{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Vairationl Inference for GPRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates implementation of GPRN (Wislon, 2011) with SVI inference for Jura dataset. Swiss Jura dataset consists of geo-spatial measurements of metals Ni Cd and Zn in the Swiss region of Jura. This can be modeled using mutli-response regression methods.\n",
    "\n",
    "The dataset in the original format can be found here :https://sites.google.com/site/goovaertspierre/pierregoovaertswebsite/download/jura-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Module and data imports **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "from scipy.cluster.vq import kmeans\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(1231)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "float_type = tf.float64\n",
    "jitter_level = 1e-5\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "# define model chekpoint path & tensorboard path \n",
    "modelPath = \"gprn.ckpt\"\n",
    "tbPath    = \"gprn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main module imports\n",
    "from onofftf.main import Param, DataSet, GaussKL, KernSE, GPConditional\n",
    "from onofftf.utils import modelmanager\n",
    "from gpflow import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trian and test splits\n",
    "traindf = pd.read_csv(\"data/train.csv\")\n",
    "testdf  = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "Xtrain = traindf[['x', 'y']].as_matrix()\n",
    "Ytrain = traindf[['cd','ni','zn']].as_matrix()\n",
    "\n",
    "Xtest = testdf[['x', 'y']].as_matrix()\n",
    "Ytest = testdf[['cd','ni','zn']].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet object gives easy access for mini-batches over epochs\n",
    "train_data = DataSet(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Set modeling parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "\n",
    "num_iter      = 50000           # optimization iterations\n",
    "num_minibatch = 220             # mini-batch size\n",
    "num_output    = Ytrain.shape[1] # number of responses\n",
    "num_latent    = 2               # number of latent functions \n",
    "num_data      = Xtrain.shape[0] # number of observations \n",
    "\n",
    "\n",
    "\n",
    "num_inducing_f = [10            # number of inducing points for latent function \\f\n",
    "                  for i in range(num_latent)]\n",
    "\n",
    "num_inducing_w = [[10           # number of inducing points for weight matrix components W\n",
    "                   for i in range(num_output)]\n",
    "                   for j in range(num_latent)]\n",
    "\n",
    "qf_diag = True  # diagnoal assumption for inducing posterior S_f\n",
    "qw_diag = True  # diagnoal assumption for inducing posterior S_w\n",
    "\n",
    "kern_param_learning_rate = 1e-2\n",
    "indp_param_learning_rate = 1e-2\n",
    "noise_param_learning_rate = 1e-3\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Parameter initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_x = lambda num_init: kmeans(Xtrain,num_init)[0]\n",
    "\n",
    "# initialize noise parameter\n",
    "init_noisevar = [0.01, 0.01,0.01]\n",
    "\n",
    "# initialize f's\n",
    "init_fkvar = np.ones((num_latent,1))*1.\n",
    "init_fkell = np.ones((num_latent,1))*1.\n",
    "\n",
    "init_Zf = [init_x(i)\n",
    "           for i in num_inducing_f]\n",
    "\n",
    "init_u_fm = [np.random.randn(num_inducing_f[i]).reshape(-1,1)*1e-2\n",
    "             for i in range(num_latent)]\n",
    "if qf_diag:\n",
    "    init_u_fs_sqrt = [np.ones(num_inducing_f[i]).reshape(1,-1).T\n",
    "                      for i in range(num_latent)]\n",
    "else:\n",
    "    init_u_fs_sqrt = [np.eye(num_inducing_f[i])\n",
    "                      for i in range(num_latent)]\n",
    "\n",
    "\n",
    "    \n",
    "# initalize W's\n",
    "init_wkvar = np.ones((num_latent,num_output))*1.\n",
    "init_wkell = np.ones((num_latent,num_output))*1.\n",
    "\n",
    "init_Zw = [[init_x(num_inducing_w[j][i])\n",
    "            for i in range(num_output)]\n",
    "            for j in range(num_latent)]\n",
    "\n",
    "init_u_wm = [[np.random.randn(num_inducing_w[j][i]).reshape(-1,1)*1e-2\n",
    "              for i in range(num_output)]\n",
    "              for j in range(num_latent)]\n",
    "if qw_diag:\n",
    "    init_u_ws_sqrt = [[np.ones(num_inducing_w[j][i]).reshape(1,-1).T\n",
    "                       for i in range(num_output)]\n",
    "                       for j in range(num_latent)]\n",
    "else:\n",
    "    init_u_ws_sqrt = [[np.eye(num_inducing_w[j][i])\n",
    "                       for i in range(num_output)]\n",
    "                       for j in range(num_latent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define parameters in TensorFlow **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for dataset\n",
    "X = tf.placeholder(dtype = float_type)\n",
    "Y = tf.placeholder(dtype = float_type)\n",
    "\n",
    "\n",
    "# kernel for latent functions : K_f\n",
    "with tf.name_scope(\"f_kern\"):\n",
    "    fkell = [Param(init_fkell[i],transform=transforms.Log1pe(),\n",
    "                   name=\"lengthscale_\"+str(i),learning_rate = kern_param_learning_rate,summ=True)\n",
    "             for i in range(num_latent)]\n",
    "\n",
    "    fkvar = [Param(init_fkvar[i],transform=transforms.Log1pe(),\n",
    "                   name=\"variance_\"+str(i),learning_rate = kern_param_learning_rate,summ=True)\n",
    "             for i in range(num_latent)]\n",
    "\n",
    "fkern_list = [KernSE(fkell[i],fkvar[i])\n",
    "              for i in range(num_latent)]\n",
    "\n",
    "# kernel for weight matrix functions : K_w\n",
    "with tf.name_scope(\"w_kern\"):\n",
    "    wkell = [[Param(init_wkell[j][i],transform=transforms.Log1pe(),\n",
    "                   name=\"lengthscale_\"+str(j)+\"_\"+str(i),learning_rate = kern_param_learning_rate,summ=True)\n",
    "             for i in range(num_output)]\n",
    "             for j in range(num_latent)]\n",
    "\n",
    "    wkvar = [[Param(init_wkvar[j][i],transform=transforms.Log1pe(),\n",
    "                   name=\"variance_\"+str(j)+\"_\"+str(i),learning_rate = kern_param_learning_rate,summ=True)\n",
    "             for i in range(num_output)]\n",
    "             for j in range(num_latent)]\n",
    "\n",
    "wkern_list = [[KernSE(wkell[j][i],wkvar[j][i])\n",
    "             for i in range(num_output)]\n",
    "             for j in range(num_latent)]\n",
    "\n",
    "# noise variance parameters\n",
    "with tf.name_scope(\"likelihood\"):\n",
    "    noisevar_list = [Param(init_noisevar[i],transform=transforms.Log1pe(),\n",
    "                     name=\"variance\"+str(i),learning_rate = noise_param_learning_rate,summ=True)\n",
    "                for i in range(num_output)]\n",
    "\n",
    "# inducing f\n",
    "with tf.name_scope(\"f_ind\"):\n",
    "    Zf_list = [Param(init_Zf[i],name=\"z_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "               for i in range(num_latent)]\n",
    "        \n",
    "    u_fm_list = [Param(init_u_fm[i],name=\"value_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "                 for i in range(num_latent)]\n",
    "    \n",
    "    if qf_diag:\n",
    "        u_fs_sqrt_list = [Param(init_u_fs_sqrt[i],transforms.positive,\n",
    "                                name=\"variance_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "                          for i in range(num_latent)]\n",
    "    else:\n",
    "        u_fs_sqrt_list = [Param(init_u_fs_sqrt[i],transforms.LowerTriangular(init_u_fs_sqrt[i].shape[0]),\n",
    "                                name=\"variance_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "                          for i in range(num_latent)]\n",
    "\n",
    "# inducing W  \n",
    "with tf.name_scope(\"w_ind\"):\n",
    "    Zw_list = [[Param(init_Zw[j][i],name=\"z_\"+str(j)+\"_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "                for i in range(num_output)]\n",
    "                for j in range(num_latent)]\n",
    "    \n",
    "    u_wm_list = [[Param(init_u_wm[j][i],name=\"value_\"+str(j)+\"_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "                 for i in range(num_output)]\n",
    "                 for j in range(num_latent)]    \n",
    "    \n",
    "    if qw_diag:\n",
    "        u_ws_sqrt_list = [[Param(init_u_ws_sqrt[j][i],transforms.positive,\n",
    "                                name=\"variance_\"+str(j)+\"_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "                          for i in range(num_output)]\n",
    "                          for j in range(num_latent)]\n",
    "    else:\n",
    "        u_ws_sqrt_list = [[Param(init_u_ws_sqrt[j][i],transforms.LowerTriangular(init_u_ws_sqrt[j][i].shape[0]),\n",
    "                                name=\"variance_\"+str(j)+\"_\"+str(i),learning_rate = indp_param_learning_rate,summ=True)\n",
    "                          for i in range(num_output)]\n",
    "                          for j in range(num_latent)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define modeling functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL divergence between all inducing priors and posteriors\n",
    "def build_prior_kl(u_fm_list,u_fs_sqrt_list,fkern_list,Zf_list,\n",
    "                   u_wm_list,u_ws_sqrt_list,wkern_list,Zw_list):\n",
    "    \n",
    "    kl_f = [GaussKL(u_fm_list[i].get_tfv(),\n",
    "                    u_fs_sqrt_list[i].get_tfv(),\n",
    "                    fkern_list[i].K(Zf_list[i].get_tfv()))\n",
    "            for i in range(num_latent)]\n",
    "\n",
    "    kl_w = [[GaussKL(u_wm_list[j][i].get_tfv(),\n",
    "                    u_ws_sqrt_list[j][i].get_tfv(),\n",
    "                    wkern_list[j][i].K(Zw_list[j][i].get_tfv()))\n",
    "            for i in range(num_output)]\n",
    "            for j in range(num_latent)]\n",
    "\n",
    "    kl = tf.add_n(kl_f) + tf.reduce_sum(tf.add_n(kl_w))\n",
    "    \n",
    "    return kl\n",
    "\n",
    "\n",
    "# Build predictive posterior\n",
    "def build_predict(Xnew,u_fm_list,u_fs_sqrt_list,fkern_list,Zf_list,\n",
    "                       u_wm_list,u_ws_sqrt_list,wkern_list,Zw_list):\n",
    "    \n",
    "    # Get conditionals for f \n",
    "    fmean_list = [[] for i in range(num_latent)]\n",
    "    fvar_list  = [[] for i in range(num_latent)]\n",
    "    for i in range(num_latent):\n",
    "        fmean_list[i], fvar_list[i] = GPConditional(Xnew,\n",
    "                                                    Zf_list[i].get_tfv(),\n",
    "                                                    fkern_list[i],\n",
    "                                                    u_fm_list[i].get_tfv(),\n",
    "                                                    full_cov=False,\n",
    "                                                    q_sqrt=u_fs_sqrt_list[i].get_tfv(),\n",
    "                                                    whiten=False)\n",
    "    fmean = tf.concat(fmean_list,axis=1)\n",
    "    fvar = tf.concat(fvar_list,axis=1)\n",
    "    \n",
    "    # get conditional for w\n",
    "    wmean_list = [[[] for i in range(num_output)]\n",
    "                      for j in range(num_latent)]\n",
    "    \n",
    "    wvar_list  = [[[] for i in range(num_output)]\n",
    "                      for j in range(num_latent)]\n",
    "    for i in range(num_output):\n",
    "        for j in range(num_latent):\n",
    "            wmean_list[j][i], wvar_list[j][i] = GPConditional(Xnew,\n",
    "                                                              Zw_list[j][i].get_tfv(),\n",
    "                                                              wkern_list[j][i],\n",
    "                                                              u_wm_list[j][i].get_tfv(),\n",
    "                                                              full_cov=False,\n",
    "                                                              q_sqrt=u_ws_sqrt_list[j][i].get_tfv(),\n",
    "                                                              whiten=False)\n",
    "    wmean = tf.concat(wmean_list,axis=2)\n",
    "    wvar = tf.concat(wvar_list,axis=2)\n",
    "\n",
    "    # compute augmented f : wf : w^T f\n",
    "    wfmean = tf.transpose(tf.reduce_sum(tf.multiply(tf.expand_dims(fmean,axis=0),wmean),axis=2))\n",
    "\n",
    "    return wfmean, fmean, fvar, wmean, wvar\n",
    "\n",
    "\n",
    "# variational expectations of likelihood\n",
    "def variational_expectations(Y, wfmean, fmean, fvar, wmean, wvar, noisevar_list):\n",
    "    N = tf.cast(tf.shape(Y)[0], float_type)\n",
    "    noisevar = tf.stack([var.get_tfv() for var in noisevar_list],axis=0)\n",
    "\n",
    "    t1 = tf.reduce_sum(tf.square(tf.subtract(Y,wfmean)),axis=0)\n",
    "    t2 = tf.reduce_sum(tf.transpose(tf.reduce_sum(tf.multiply(tf.expand_dims(fvar,axis=0),tf.square(wmean)),axis=2)),axis=0)\n",
    "    t3 = tf.reduce_sum(tf.transpose(tf.reduce_sum(tf.multiply(tf.expand_dims(tf.square(fmean),axis=0),wvar),axis=2)),axis=0)\n",
    "    t4 = tf.reduce_sum(tf.transpose(tf.reduce_sum(tf.multiply(tf.expand_dims(fvar,axis=0),wvar),axis=2)),axis=0)\n",
    "\n",
    "    tsum = tf.divide(t1 + t2 + t3 + t4,noisevar)\n",
    "\n",
    "    return - 0.5 * N * tf.reduce_sum(tf.log(2 * np.pi * noisevar)) \\\n",
    "           - tf.reduce_sum(0.5 * tsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build lowerbound**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build prior\n",
    "with tf.name_scope(\"kl\"):\n",
    "    kl = build_prior_kl(u_fm_list,u_fs_sqrt_list,fkern_list,Zf_list,\n",
    "                        u_wm_list,u_ws_sqrt_list,wkern_list,Zw_list)\n",
    "    tf.summary.scalar('kl', kl)\n",
    "\n",
    "# build predict function\n",
    "with tf.name_scope(\"model_build\"):\n",
    "    wfmean, fmean, fvar, wmean, wvar = build_predict(X,u_fm_list,u_fs_sqrt_list,fkern_list,Zf_list,\n",
    "                                                     u_wm_list,u_ws_sqrt_list,wkern_list,Zw_list)\n",
    "    tf.summary.histogram('wfmean',wfmean)\n",
    "    tf.summary.histogram('fmean',fmean)\n",
    "    tf.summary.histogram('fvar',fvar)\n",
    "    tf.summary.histogram('wmean',wmean)\n",
    "    tf.summary.histogram('wvar',wvar)\n",
    "\n",
    "# compute likelihood\n",
    "with tf.name_scope(\"var_exp\"):\n",
    "    var_exp = variational_expectations(Y, wfmean, fmean, fvar, wmean, wvar, noisevar_list)\n",
    "    tf.summary.scalar('var_exp', var_exp)\n",
    "\n",
    "    # mini-batch scaling\n",
    "    scale =  tf.cast(num_data, float_type) / tf.cast(num_minibatch, float_type)\n",
    "    var_exp_scaled = var_exp * scale\n",
    "    tf.summary.scalar('var_exp_scaled', var_exp_scaled)\n",
    "\n",
    "# final lower bound\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost =  -(var_exp_scaled - kl)\n",
    "    tf.summary.scalar('cost',cost)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run model optimization op**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name f_kern/lengthscale_0:0/gradient is illegal; using f_kern/lengthscale_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_kern/lengthscale_1:0/gradient is illegal; using f_kern/lengthscale_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_kern/variance_0:0/gradient is illegal; using f_kern/variance_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_kern/variance_1:0/gradient is illegal; using f_kern/variance_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/lengthscale_0_0:0/gradient is illegal; using w_kern/lengthscale_0_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/lengthscale_0_1:0/gradient is illegal; using w_kern/lengthscale_0_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/lengthscale_0_2:0/gradient is illegal; using w_kern/lengthscale_0_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/lengthscale_1_0:0/gradient is illegal; using w_kern/lengthscale_1_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/lengthscale_1_1:0/gradient is illegal; using w_kern/lengthscale_1_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/lengthscale_1_2:0/gradient is illegal; using w_kern/lengthscale_1_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/variance_0_0:0/gradient is illegal; using w_kern/variance_0_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/variance_0_1:0/gradient is illegal; using w_kern/variance_0_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/variance_0_2:0/gradient is illegal; using w_kern/variance_0_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/variance_1_0:0/gradient is illegal; using w_kern/variance_1_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/variance_1_1:0/gradient is illegal; using w_kern/variance_1_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_kern/variance_1_2:0/gradient is illegal; using w_kern/variance_1_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_ind/z_0:0/gradient is illegal; using f_ind/z_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_ind/z_1:0/gradient is illegal; using f_ind/z_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_ind/value_0:0/gradient is illegal; using f_ind/value_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_ind/value_1:0/gradient is illegal; using f_ind/value_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_ind/variance_0:0/gradient is illegal; using f_ind/variance_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name f_ind/variance_1:0/gradient is illegal; using f_ind/variance_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/z_0_0:0/gradient is illegal; using w_ind/z_0_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/z_0_1:0/gradient is illegal; using w_ind/z_0_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/z_0_2:0/gradient is illegal; using w_ind/z_0_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/z_1_0:0/gradient is illegal; using w_ind/z_1_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/z_1_1:0/gradient is illegal; using w_ind/z_1_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/z_1_2:0/gradient is illegal; using w_ind/z_1_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/value_0_0:0/gradient is illegal; using w_ind/value_0_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/value_0_1:0/gradient is illegal; using w_ind/value_0_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/value_0_2:0/gradient is illegal; using w_ind/value_0_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/value_1_0:0/gradient is illegal; using w_ind/value_1_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/value_1_1:0/gradient is illegal; using w_ind/value_1_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/value_1_2:0/gradient is illegal; using w_ind/value_1_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/variance_0_0:0/gradient is illegal; using w_ind/variance_0_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/variance_0_1:0/gradient is illegal; using w_ind/variance_0_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/variance_0_2:0/gradient is illegal; using w_ind/variance_0_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/variance_1_0:0/gradient is illegal; using w_ind/variance_1_0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/variance_1_1:0/gradient is illegal; using w_ind/variance_1_1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_ind/variance_1_2:0/gradient is illegal; using w_ind/variance_1_2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name likelihood/variance0:0/gradient is illegal; using likelihood/variance0_0/gradient instead.\n",
      "INFO:tensorflow:Summary name likelihood/variance1:0/gradient is illegal; using likelihood/variance1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name likelihood/variance2:0/gradient is illegal; using likelihood/variance2_0/gradient instead.\n"
     ]
    }
   ],
   "source": [
    "all_var_list = tf.trainable_variables()\n",
    "all_lr_list = [var._learning_rate for var in all_var_list]\n",
    "\n",
    "train_opt_group = []\n",
    "\n",
    "for group_learning_rate in set(all_lr_list):\n",
    "    _ind_bool = np.where(np.isin(np.array(all_lr_list),group_learning_rate))[0]\n",
    "    group_var_list = [all_var_list[ind] for ind in _ind_bool]\n",
    "    group_tf_optimizer = tf.train.AdamOptimizer(learning_rate = group_learning_rate)\n",
    "    group_grad_list = tf.gradients(cost,group_var_list)\n",
    "    group_grads_and_vars = list(zip(group_grad_list,group_var_list))\n",
    "    \n",
    "    \n",
    "    group_train_op = group_tf_optimizer.apply_gradients(group_grads_and_vars)\n",
    "    \n",
    "    # Summarize all gradients\n",
    "    for grad, var in group_grads_and_vars:\n",
    "        tf.summary.histogram(var.name + '/gradient', grad)\n",
    "    \n",
    "    train_opt_group.append({'names':[var.name for var in group_var_list],\n",
    "                            'vars':group_var_list,\n",
    "                            'learning_rate':group_learning_rate,\n",
    "                            'grads':group_grad_list,\n",
    "                            'train_op':group_train_op})\n",
    "    \n",
    "train_op = tf.group(*[group['train_op'] for group in train_opt_group])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# model saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# tensorboard summary\n",
    "summ_merged = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(tbPath,\n",
    "                                        graph=sess.graph)\n",
    "  \n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       iteration        objective          var_exp               kl\n",
      "               0     89626549.689    -76130619.101           48.112\n",
      "            1000      4197773.730     -3526928.382        45617.135\n",
      "            2000      3114582.870     -2610188.533        41679.097\n",
      "            3000      1966232.773     -1639055.669        36617.235\n",
      "            4000      1445957.425     -1201887.271        31008.320\n",
      "            5000      1018696.645      -843924.129        25167.784\n",
      "            6000       752837.374      -622227.147        20306.324\n",
      "            7000       454190.262      -371785.048        16497.865\n",
      "            8000       288976.306      -234505.816        12899.004\n",
      "            9000       187240.044      -150585.047         9960.375\n",
      "           10000       134413.608      -107469.723         7892.434\n",
      "           11000       109370.042       -87511.394         6345.265\n",
      "           12000        77195.701       -61194.670         5152.885\n",
      "           13000        60229.166       -47468.281         4346.054\n",
      "           14000        45483.564       -35538.862         3644.630\n",
      "           15000        33592.813       -25869.996         3136.772\n",
      "           16000        34891.460       -27336.270         2709.215\n",
      "           17000        24658.443       -18939.110         2361.946\n",
      "           18000        20987.244       -16101.540         2031.340\n",
      "           19000        16877.411       -12825.267         1778.575\n",
      "           20000        15842.958       -12140.305         1550.508\n",
      "           21000        13397.134       -10222.915         1361.975\n",
      "           22000        12287.480        -9385.693         1237.959\n"
     ]
    }
   ],
   "source": [
    "print('{:>16s}'.format(\"iteration\"),'{:>16s}'.format(\"objective\"),'{:>16s}'.format(\"var_exp\"),'{:>16s}'.format(\"kl\"))\n",
    "\n",
    "for i in range(num_iter):\n",
    "    batch = train_data.next_batch(num_minibatch)\n",
    "    try:    \n",
    "        summary,_ = sess.run([summ_merged,train_op],feed_dict={X : batch[0],Y : batch[1]})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            _cost    = cost.eval({X : batch[0],Y : batch[1]})\n",
    "            _var_exp = var_exp.eval({X : batch[0],Y :batch[1]})\n",
    "            _kl      = kl.eval({X : batch[0],Y : batch[1]})\n",
    "            print('{:>16d}'.format(i),'{:>16.3f}'.format(_cost),'{:>16.3f}'.format(_var_exp),'{:>16.3f}'.format(_kl))\n",
    "            \n",
    "            if i > 200:\n",
    "                summary_writer.add_summary(summary,i)\n",
    "                summary_writer.flush()\n",
    "\n",
    "       \n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"Stopping training\")\n",
    "        break\n",
    "        \n",
    "modelmngr = modelmanager(saver, sess, modelPath)\n",
    "modelmngr.save()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predictions on trainset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "pred_trn_wfmean = wfmean.eval({X:Xtrain})\n",
    "pred_trn_fmean = fmean.eval({X:Xtrain})\n",
    "pred_trn_fvar = fvar.eval({X:Xtrain})\n",
    "pred_trn_wmean = wmean.eval({X:Xtrain})\n",
    "pred_trn_wvar =wvar.eval({X:Xtrain})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Grid plot of the model fit **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12,6)\n",
    "\n",
    "xplot, yplot = np.meshgrid(np.linspace(0.1,6.1,30),np.linspace(0.1,6.1,30))\n",
    "Xplot = np.concatenate([xplot.reshape(-1,1),yplot.reshape(-1,1)],axis=1)\n",
    "\n",
    "# predictions\n",
    "pred_plt_wfmean = wfmean.eval({X:Xplot})\n",
    "pred_plt_fmean = fmean.eval({X:Xplot})\n",
    "pred_plt_fvar = fvar.eval({X:Xplot})\n",
    "pred_plt_wmean = wmean.eval({X:Xplot})\n",
    "pred_plt_wvar =wvar.eval({X:Xplot})\n",
    "\n",
    "# inducing locations and values\n",
    "ind_plot_f = [Zf_list[i].get_tfv().eval() \n",
    "              for i in range(num_latent)]\n",
    "ind_plot_w = [[Zw_list[j][i].get_tfv().eval()\n",
    "              for i in range(num_output)]\n",
    "              for j in range(num_latent)]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# plot latent functions\n",
    "gs00 = gridspec.GridSpec(num_latent, 1)\n",
    "for lf in np.arange(num_latent):\n",
    "    ax   = fig.add_subplot(gs00[lf,0], aspect='equal')\n",
    "    cax  = ax.contourf(xplot, yplot, pred_plt_fmean[:,lf].reshape(xplot.shape),alpha=0.7,cmap='RdYlBu')\n",
    "    cbar = fig.colorbar(cax,extend='max',fraction=0.046, pad=0.04)\n",
    "    ax.scatter(ind_plot_f[lf][:,0],ind_plot_f[lf][:,1],s=10)\n",
    "    ax.set_xlim(0.1,6.1)\n",
    "    ax.set_ylim(0.1,6.1)\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "    if lf == 0:\n",
    "        ax.set_title(\"latent functions\")\n",
    "gs00.tight_layout(fig, rect=[0, 0, 0.2, 1])\n",
    "\n",
    "# plot weight functions\n",
    "gs01 = gridspec.GridSpec(num_output,num_latent)\n",
    "for lf in np.arange(num_latent):\n",
    "    for wf in np.arange(num_output):\n",
    "        ax   = fig.add_subplot(gs01[wf,lf],adjustable='box-forced')\n",
    "        cax  = ax.contourf(xplot, yplot,pred_plt_wmean[wf,:,lf].reshape(xplot.shape),alpha=0.7,cmap='RdYlBu')\n",
    "        cbar = fig.colorbar(cax,extend='max',fraction=0.046, pad=0.04)\n",
    "        ax.scatter(ind_plot_w[lf][wf][:,0],ind_plot_w[lf][wf][:,1],s=10)\n",
    "        ax.set_xlim(0.1,6.1)\n",
    "        ax.set_ylim(0.1,6.1)\n",
    "        ax.xaxis.set_ticks([])\n",
    "        ax.yaxis.set_ticks([])\n",
    "        if lf == 0 and wf == 0:\n",
    "            ax.set_title(\"weight functions\")\n",
    "gs01.tight_layout(fig, rect=[0.2, 0, 0.6,1],h_pad=0.5)\n",
    "\n",
    "# plot output functions\n",
    "gs02 = gridspec.GridSpec(num_output, 1)\n",
    "for wf in np.arange(num_output):\n",
    "    ax   = fig.add_subplot(gs02[wf,0], aspect='equal',adjustable='box-forced')\n",
    "    cax  = ax.contourf(xplot, yplot,pred_plt_wfmean[:,wf].reshape(xplot.shape),alpha=0.7)\n",
    "    cbar = fig.colorbar(cax,extend='max',fraction=0.046, pad=0.04)\n",
    "    ax.set_xlim(0.1,6.1)\n",
    "    ax.set_ylim(0.1,6.1)\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "    if wf == 0:\n",
    "        ax.set_title(\"predictions\")\n",
    "\n",
    "\n",
    "gs02.tight_layout(fig, rect=[0.6, 0, 0.8, 1],h_pad=0.5)\n",
    "\n",
    "# plot data functions\n",
    "gs03 = gridspec.GridSpec(num_output, 1)\n",
    "for wf in np.arange(num_output):\n",
    "    ax   = fig.add_subplot(gs03[wf,0], aspect='equal',adjustable='box-forced')\n",
    "    cax  = ax.scatter(Xtrain[:,0],Xtrain[:,1],c= Ytrain[:,wf],alpha=0.7,s=10)\n",
    "    cbar = fig.colorbar(cax,extend='max',fraction=0.046, pad=0.04)\n",
    "    ax.set_xlim(0.1,6.1)\n",
    "    ax.set_ylim(0.1,6.1)\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "    if wf == 0:\n",
    "        ax.set_title(\"actual\")\n",
    "\n",
    "gs03.tight_layout(fig, rect=[0.8, 0, 1, 1],h_pad=0.5)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train set predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12,6)\n",
    "fig, ax = plt.subplots(2,3,sharex='all', sharey='all')\n",
    "\n",
    "names = [\"Cd\",\"Ni\",\"Zn\"]\n",
    "for i in range(3):\n",
    "    cnorm = mpl.colors.Normalize(vmin=0.,vmax=Ytrain[:,i].max())\n",
    "    \n",
    "    cx1 = ax[0,i].scatter(Xtrain[:,0],Xtrain[:,1],c=Ytrain[:,i]) #,norm=cnorm\n",
    "    cbar1 = fig.colorbar(cx1,ax=ax[0,i],fraction=0.046, pad=0.04)\n",
    "    ax[0,i].set_title(\"actutal - \"+ names[i])\n",
    "    ax[0,i].xaxis.set_ticks([])\n",
    "    ax[0,i].yaxis.set_ticks([])\n",
    "\n",
    "    cx2 = ax[1,i].scatter(Xtrain[:,0],Xtrain[:,1],c=pred_trn_wfmean[:,i]) #,norm=cnorm\n",
    "    cbar2 = fig.colorbar(cx2,ax=ax[1,i],fraction=0.046, pad=0.04)\n",
    "    ax[1,i].set_title(\"predicted - \"+ names[i]+\"  noise var:\"+str(np.round(noisevar_list[i].get_tfv().eval(),3)))\n",
    "    ax[1,i].xaxis.set_ticks([])\n",
    "    ax[1,i].yaxis.set_ticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train mse:\",np.sqrt(np.mean((pred_trn_wfmean - Ytrain)**2,axis=0)))\n",
    "print(\"train mae:\",np.mean(np.abs(pred_trn_wfmean - Ytrain),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test set predictions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12,6)\n",
    "fig, ax = plt.subplots(2,3,sharex='all', sharey='all')\n",
    "\n",
    "pred_tst_wfmean = wfmean.eval({X:Xtest})\n",
    "\n",
    "names = [\"Cd\",\"Ni\",\"Zn\"]\n",
    "for i in range(3):\n",
    "    cx1 = ax[0,i].scatter(Xtest[:,0],Xtest[:,1],c=Ytest[:,i])\n",
    "    cbar1 = fig.colorbar(cx1,ax=ax[0,i],fraction=0.046, pad=0.04)\n",
    "    ax[0,i].set_title(\"actutal - \"+ names[i])\n",
    "    ax[0,i].xaxis.set_ticks([])\n",
    "    ax[0,i].yaxis.set_ticks([])\n",
    "\n",
    "    cx2 = ax[1,i].scatter(Xtest[:,0],Xtest[:,1],c=pred_tst_wfmean[:,i])\n",
    "    cbar2 = fig.colorbar(cx2,ax=ax[1,i],fraction=0.046, pad=0.04)\n",
    "    ax[1,i].set_title(\"predicted - \"+ names[i]+\"  noise var:\"+str(np.round(noisevar_list[i].get_tfv().eval(),3)))\n",
    "    ax[1,i].xaxis.set_ticks([])\n",
    "    ax[1,i].yaxis.set_ticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf13]",
   "language": "python",
   "name": "conda-env-tf13-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
